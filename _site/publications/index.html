<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="images\group.jpg">
  <title>Relational ML - Publications</title>
  <meta name="description" content="Relational ML -- Publications.">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/publications/">
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Signika Negative:wght@300&display=swap">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
  <script src="/js/common.js"></script>
  <script src="/js/bootstrap.min.js"></script>
  
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WLRLV6PPVE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WLRLV6PPVE');
</script>

</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>
	
	<img src="/images/logo/Logo_RML.png" style="width:40px; height: auto; margin-top: 5px; margin-right: 10px; float: left; margin-bottom: -10px;"/>
    <a class="navbar-brand" href="/">Relational ML Lab</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="/">Home</a></li>
		<li><a href="/team">Team</a></li>
		<li><a href="/openings">Openings</a></li>
		<li><a href="/research">Research</a></li>
		<li><a href="/publications">Publications</a></li>
		<li><a href="https://github.com/relationalml/">
			<!-- github icon by Free Icons (https://free-icons.github.io/free-icons/) -->
			<svg xmlns="http://www.w3.org/2000/svg" height="1em" fill="currentColor" viewBox="0 0 496 484" style="vertical-align: middle;">
			  <path d="M 166 389 Q 165 393 161 393 Q 155 393 155 389 Q 156 386 160 386 Q 165 386 166 389 L 166 389 Z M 135 385 Q 134 388 139 390 Q 144 391 145 388 Q 146 384 141 383 Q 136 382 135 385 L 135 385 Z M 179 383 Q 174 384 174 388 Q 175 391 180 391 Q 185 389 185 386 Q 184 383 179 383 L 179 383 Z M 245 0 Q 138 2 70 70 L 70 70 Q 2 138 0 244 Q 1 329 47 393 Q 93 457 170 483 Q 188 484 187 471 Q 187 467 187 456 Q 187 433 187 410 Q 185 410 168 412 Q 150 413 130 408 Q 110 402 102 380 Q 102 378 94 364 Q 86 351 74 343 Q 72 342 66 335 Q 59 329 76 328 Q 77 327 90 332 Q 103 336 114 354 Q 132 381 153 381 Q 175 381 187 375 Q 191 351 203 341 Q 159 339 126 319 Q 93 300 91 230 Q 91 210 97 197 Q 102 184 114 172 Q 112 166 110 148 Q 108 130 117 104 Q 135 101 159 115 Q 184 128 186 131 Q 186 131 186 131 Q 216 122 249 122 Q 281 122 312 131 Q 312 130 325 122 Q 337 114 353 108 Q 369 101 381 104 Q 390 130 388 148 Q 386 166 383 172 Q 395 184 402 197 Q 409 210 409 230 Q 408 278 392 301 Q 375 323 349 331 Q 323 339 294 341 Q 310 352 311 387 Q 311 424 311 453 Q 311 466 311 471 Q 310 484 328 483 Q 404 457 450 393 Q 495 329 496 244 Q 495 174 462 119 Q 428 64 372 32 Q 315 1 245 0 L 245 0 Z M 97 345 Q 95 347 98 350 Q 101 353 103 351 Q 105 349 102 346 Q 100 343 97 345 L 97 345 Z M 86 337 Q 86 339 89 341 Q 92 342 93 340 Q 94 338 91 336 Q 88 335 86 337 L 86 337 Z M 119 372 Q 117 375 120 379 Q 124 382 127 380 Q 128 377 125 373 Q 121 370 119 372 L 119 372 Z M 107 358 Q 105 360 107 364 Q 110 367 113 366 Q 115 364 113 360 Q 110 356 107 358 L 107 358 Z" />
			</svg>
			</a></li>
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="publications" class="col-sm-12">
  <h1 id="publications">Publications</h1>

<p><select id="memberselect">
    <option value="">All members</option>
    <option value="Burkholz">Dr. Rebekka Burkholz</option>
    <option value="Gadhikar">Advait Gadhikar</option>
    <option value="Mustafa">Nimrah Mustafa</option>
    <option value="Jamadandi">Adarsh Jamadandi</option>
    <option value="Nelaturu">Harsha Nelaturu</option>
    <option value="Rubio-Madrigal">Celia Rubio-Madrigal</option>
    <option value="Jacobs">Tom Jacobs</option>
    </select></p>

<ol class="bibliography"><li><!-- _layouts/bib.html -->


  <div class="row" style="margin-top: 30px;" id="bib-gadhikar2024masks" authors="GadhikarBurkholz">
        <div class="preview"><img class="preview z-depth-1 rounded" src="/images/publications/default.png" /></div>
        <!-- Entry bib key -->
        <div id="gadhikar2024masks" class="col-sm-10">
        <!-- Title -->
        
          <div class="title"><a href="https://openreview.net/forum?id=qODvxQ8TXW"><b>Masks, Signs, And Learning Rate Rewinding</b></a></div>
        
        <!-- Author -->
        <div class="author">
        

        <b>Advait Harshal Gadhikar</b>,&nbsp;and&nbsp;<b>Rebekka Burkholz</b></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In The Twelfth International Conference on Learning Representations</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
        <div class="links"><a class="conf btn btn-sm z-depth-0">ICLR</a><a class="yellow btn btn-sm z-depth-0">Spotlight</a><a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openreview.net/pdf?id=qODvxQ8TXW" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape initially problematic sign configurations.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gadhikar2024masks</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Masks, Signs, And Learning Rate Rewinding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gadhikar, Advait Harshal and Burkholz, Rebekka}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=qODvxQ8TXW}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->


  <div class="row" style="margin-top: 30px;" id="bib-burkholz2024batch" authors="Burkholz">
        <div class="preview"><img class="preview z-depth-1 rounded" src="/images/publications/default.png" /></div>
        <!-- Entry bib key -->
        <div id="burkholz2024batch" class="col-sm-10">
        <!-- Title -->
        
          <div class="title"><a href="https://openreview.net/forum?id=wOSYMHfENq"><b>Batch normalization is sufficient for universal function approximation in CNNs</b></a></div>
        
        <!-- Author -->
        <div class="author">
        

        <b>Rebekka Burkholz</b></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In The Twelfth International Conference on Learning Representations</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
        <div class="links"><a class="conf btn btn-sm z-depth-0">ICLR</a><a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openreview.net/pdf?id=wOSYMHfENq" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Layer normalization, for which Batch Normalization (BN) is a popular choice, is an integral part of many deep learning architectures and contributes significantly to the learning success. We provide a partial explanation for this phenomenon by proving that training normalization layers alone is already sufficient for universal function approximation if the number of available, potentially random features matches or exceeds the weight parameters of the target networks that can be expressed. Our bound on the number of required features does not only improve on a recent result for fully-connected feed-forward architectures but also applies to CNNs with and without residual connections and almost arbitrary activation functions (which include ReLUs). Our explicit construction of a given target network solves a depth-width trade-off that is driven by architectural constraints and can explain why switching off entire neurons can have representational benefits, as has been observed empirically. To validate our theory, we explicitly match target networks that outperform experimentally obtained networks with trained BN parameters by utilizing a sufficient number of random features.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">burkholz2024batch</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Batch normalization is sufficient for universal function approximation in {CNN}s}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Burkholz, Rebekka}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Twelfth International Conference on Learning Representations}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=wOSYMHfENq}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->


  <div class="row" style="margin-top: 30px;" id="bib-mustafa2023are" authors="MustafaBojchevskiBurkholz">
        <div class="preview"><img class="preview z-depth-1 rounded" src="/images/publications/default.png" /></div>
        <!-- Entry bib key -->
        <div id="mustafa2023are" class="col-sm-10">
        <!-- Title -->
        
          <div class="title"><a href="https://openreview.net/forum?id=qY7UqLoora"><b>Are GATs Out of Balance?</b></a></div>
        
        <!-- Author -->
        <div class="author">
        

        <b>Nimrah Mustafa</b>,&nbsp;Aleksandar Bojchevski,&nbsp;and&nbsp;<b>Rebekka Burkholz</b></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
        <div class="links"><a class="conf btn btn-sm z-depth-0">NeurIPS</a><a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://openreview.net/pdf?id=qY7UqLoora" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/RelationalML/GAT_Balanced_Initialization" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node’s neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mustafa2023are</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Are {GAT}s Out of Balance?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mustafa, Nimrah and Bojchevski, Aleksandar and Burkholz, Rebekka}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Thirty-seventh Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=qY7UqLoora}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->


  <div class="row" style="margin-top: 30px;" id="bib-pmlr-v202-gadhikar23a" authors="GadhikarMukherjeeBurkholz">
        <div class="preview"><img class="preview z-depth-1 rounded" src="/images/publications/default.png" /></div>
        <!-- Entry bib key -->
        <div id="pmlr-v202-gadhikar23a" class="col-sm-10">
        <!-- Title -->
        
          <div class="title"><a href="https://proceedings.mlr.press/v202/gadhikar23a.html"><b>Why Random Pruning Is All We Need to Start Sparse</b></a></div>
        
        <!-- Author -->
        <div class="author">
        

        <b>Advait Harshal Gadhikar</b>,&nbsp;Sohom Mukherjee,&nbsp;and&nbsp;<b>Rebekka Burkholz</b></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 40th International Conference on Machine Learning</em>, 23–29 jul 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
        <div class="links"><a class="conf btn btn-sm z-depth-0">ICML</a><a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.mlr.press/v202/gadhikar23a/gadhikar23a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/RelationalML/sparse_to_sparse" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Random masks define surprisingly effective sparse neural network models, as has been shown empirically. The resulting sparse networks can often compete with dense architectures and state-of-the-art lottery ticket pruning algorithms, even though they do not rely on computationally expensive prune-train iterations and can be drawn initially without significant computational overhead. We offer a theoretical explanation of how random masks can approximate arbitrary target networks if they are wider by a logarithmic factor in the inverse sparsity 1 / \log(1/\textsparsity). This overparameterization factor is necessary at least for 3-layer random networks, which elucidates the observed degrading performance of random networks at higher sparsity. At moderate to high sparsity levels, however, our results imply that sparser networks are contained within random source networks so that any dense-to-sparse training scheme can be turned into a computationally more efficient sparse-to-sparse one by constraining the search to a fixed random mask. We demonstrate the feasibility of this approach in experiments for different pruning methods and propose particularly effective choices of initial layer-wise sparsity ratios of the random source network. As a special case, we show theoretically and experimentally that random source networks also contain strong lottery tickets.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pmlr-v202-gadhikar23a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Why Random Pruning Is All We Need to Start Sparse}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gadhikar, Advait Harshal and Mukherjee, Sohom and Burkholz, Rebekka}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 40th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10542--10570}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{202}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="s">{23--29 Jul}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v202/gadhikar23a.html}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->


  <div class="row" style="margin-top: 30px;" id="bib-NEURIPS2022_76bf7786" authors="Burkholz">
        <div class="preview"><img class="preview z-depth-1 rounded" src="/images/publications/default.png" /></div>
        <!-- Entry bib key -->
        <div id="NEURIPS2022_76bf7786" class="col-sm-10">
        <!-- Title -->
        
          <div class="title"><a href="https://papers.nips.cc/paper_files/paper/2022/hash/76bf7786d311217077bc8bb021946cd9-Abstract-Conference.html"><b>Most Activation Functions Can Win the Lottery Without Excessive Depth</b></a></div>
        
        <!-- Author -->
        <div class="author">
        

        <b>Rebekka Burkholz</b></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Advances in Neural Information Processing Systems</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
        <div class="links"><a class="conf btn btn-sm z-depth-0">NeurIPS</a><a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/76bf7786d311217077bc8bb021946cd9-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/RelationalML/LT-existence" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The strong lottery ticket hypothesis has highlighted the potential for training deep neural networks by pruning, which has inspired interesting practical and theoretical insights into how neural networks can represent functions. For networks with ReLU activation functions, it has been proven that a target network with depth L can be approximated by the subnetwork of a randomly initialized neural network that has double the target’s depth 2L and is wider by a logarithmic factor. We show that a depth L+1 is sufficient. This result indicates that we can expect to find lottery tickets at realistic, commonly used depths while only requiring logarithmic overparametrization. Our novel construction approach applies to a large class of activation functions and is not limited to ReLUs. Code is available on Github (RelationalML/LT-existence).</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">NEURIPS2022_76bf7786</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Burkholz, Rebekka}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{18707--18720}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Curran Associates, Inc.}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Most Activation Functions Can Win the Lottery Without Excessive Depth}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://papers.nips.cc/paper_files/paper/2022/hash/76bf7786d311217077bc8bb021946cd9-Abstract-Conference.html}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{35}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->


  <div class="row" style="margin-top: 30px;" id="bib-pmlr-v162-burkholz22a" authors="Burkholz">
        <div class="preview"><img class="preview z-depth-1 rounded" src="/images/publications/default.png" /></div>
        <!-- Entry bib key -->
        <div id="pmlr-v162-burkholz22a" class="col-sm-10">
        <!-- Title -->
        
          <div class="title"><a href="https://proceedings.mlr.press/v162/burkholz22a.html"><b>Convolutional and Residual Networks Provably Contain Lottery Tickets</b></a></div>
        
        <!-- Author -->
        <div class="author">
        

        <b>Rebekka Burkholz</b></div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 39th International Conference on Machine Learning</em>, 17–23 jul 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
        <div class="links"><a class="conf btn btn-sm z-depth-0">ICML</a><a class="yellow btn btn-sm z-depth-0">Spotlight</a><a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://proceedings.mlr.press/v162/burkholz22a/burkholz22a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The Lottery Ticket Hypothesis continues to have a profound practical impact on the quest for small scale deep neural networks that solve modern deep learning tasks at competitive performance. These lottery tickets are identified by pruning large randomly initialized neural networks with architectures that are as diverse as their applications. Yet, theoretical insights that attest their existence have been mostly focused on deed fully-connected feed forward networks with ReLU activation functions. We prove that also modern architectures consisting of convolutional and residual layers that can be equipped with almost arbitrary activation functions can contain lottery tickets with high probability.</p>
          </div><!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pmlr-v162-burkholz22a</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Convolutional and Residual Networks Provably Contain Lottery Tickets}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Burkholz, Rebekka}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 39th International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2414--2433}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{162}</span><span class="p">,</span>
  <span class="na">series</span> <span class="p">=</span> <span class="s">{Proceedings of Machine Learning Research}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="s">{17--23 Jul}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{PMLR}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://proceedings.mlr.press/v162/burkholz22a.html}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div></li></ol>
  
  <br>
</div>

<script>
document.addEventListener('DOMContentLoaded', function() {
    var memberSelect = document.getElementById('memberselect');
    if (memberSelect) {
        memberSelect.addEventListener('change', function() {
            var member = this.value;
            var entries = document.querySelectorAll('.row');
            entries.forEach(function(entry) {
                if (entry.id.includes('bib-')) {
                    var authors = entry.getAttribute('authors');
                    if (authors.includes(member)) {
                        entry.style.display = 'flex';
                    } else {
                        entry.style.display = 'none';
                    }
                }
            });
        });
    }
});
</script>
      </div>
    </div>

    <div id="footer" class="panel" style="margin-bottom: 1px;">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-6">
			<p>&copy 2024 Relational ML Lab.</p>
			<p>We are part of the <a href="https://cispa.de//">CISPA Helmholtz Center for Information Security</a>, at the <a href="https://www.uni-saarland.de/">Saarland University</a> campus.</p>
		</div>
		<div class="col-sm-6">
		  <p>Contact:</p>
		  <p>Stuhlsatzenhaus 5, 66123 Saarbrücken, Germany<br>
            (<a href="https://goo.gl/maps/KpzD1sq7HrvULVn7A">Maps</a>, <a href="https://cispa.de/en/contact#location">Directions</a>)</p>
		</div>
	  </div>
	</div>
  </div>
</div>

  </body>

</html>
